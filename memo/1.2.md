## 1.2 確率論

- 経験誤差: モデルの学習に使ったデータに対するモデルの誤差
- 汎化誤差: モデルの学習に使わなかったデータに対するモデルの誤差

#### 問題設定

- 赤と青の箱がある。
- 赤の箱にはリンゴとオレンジがそれぞれ2個,6個入っている。
- 青の箱にはリンゴとオレンジがそれぞれ3個,1個入っている。
- まず箱を1つ選び、選んだ箱からさらに果物を1つ選ぶ。
- 箱の選び方は、赤が選ばれる確率 : 青が選ばれる確率 = 4:6 となるように選ぶ。
- 果物の選び方は一様。

#### 問題: 選ばれた果物がオレンジだった時、選んだ箱が赤/青である確率は？

- ベイズの定理: `P(X|Y) * P(Y) = P(X,Y) = P(Y|X) * P(X)`
  - よって、`P(X|Y) = P(Y|X) P(X) / P(Y)`
- `P(赤) = 2/5, P(青)  = 3/5`
- `P(オレンジ|赤) = 3/4, P(オレンジ|青) = 1/4`
- `P(オレンジ, 赤) = 6/20, P(オレンジ, 青) = 3/20`
- `P(オレンジ) = 9/20`
- `P(赤|オレンジ) = P(赤, オレンジ) / P(オレンジ) = (6/20) / (9/20) = 2/3`
- `P(青|オレンジ) = P(青, オレンジ) / P(オレンジ) = (3/20) / (9/20) = 1/3`

### 1.2.2

Markdown Math のテスト。

$$
\mathbb{E}[f] =  \sum_x p(x)f(x) \\
\mathbb{E}[f] =  \int dx p(x)f(x) \\
var[f] = \mathbb{E}[ (f(x) - \mathbb{E}[f])^2 ] \\
= \sum_x p(x) \left( f(x) - \mathbb{E}[f] \right)^2 \\
= \sum_x p(x) \left( f(x) - \sum_x p(x)f(x) \right)^2 \\
= \sum_x p(x) \left( f(x)^2 - 2f(x)\sum_x p(x)f(x) + \left(\sum_x p(x)f(x)\right)^2 \right)\\
= \left( \sum_x p(x)f(x)^2 \right) - 2\left(\sum_x p(x)f(x)\right)\left(\sum_x p(x)f(x)\right) + \left( \sum_x p(x)\right)\left(\sum_x p(x)f(x)\right)^2\\
= \left( \sum_x p(x)f(x)^2 \right) - 2\left(\sum_x p(x)f(x)\right)^2 + \left(\sum_x p(x)f(x)\right)^2\\
= \left( \sum_x p(x)f(x)^2 \right) - \left(\sum_x p(x)f(x)\right)^2\\
= \mathbb{E}[f^2] - \mathbb{E}[f]^2
$$

#### 共分散

$$
\rm{cov}[x,y] = \mathbb{E}_{x,y}[\left(x - \mathbb{E}[x]\right)\left(y - \mathbb{E}[y]\right)]
$$

特に

$$
\rm{cov}[x,x] = \rm{var}[x].
$$

$$
\mathbb{E}_{x,y}[\left(x - \mathbb{E}[x]\right)\left(y - \mathbb{E}[y]\right)]\\
= \sum_{x,y}p(x,y)\left(x - \mathbb{E}[x]\right)\left(y - \mathbb{E}[y]\right)\\
= \sum_{x,y}p(x,y)\left(xy - y\mathbb{E}[x] - x\mathbb{E}[y] + \mathbb{E}[x]\mathbb{E}[y]\right)\\
= \sum_{x,y}p(x,y)xy - \mathbb{E}[x]\sum_{x,y}p(x,y)y - \mathbb{E}[y]\sum_{x,y}p(x,y)x + \mathbb{E}[x]\mathbb{E}[y]\\
= \sum_{x,y}p(x,y)xy - \mathbb{E}[x]\sum_yp(y)y - \mathbb{E}[y]\sum_xp(x)x + \mathbb{E}[x]\mathbb{E}[y]\\
= \sum_{x,y}p(x,y)xy - \mathbb{E}[x]\mathbb{E}[y] - \mathbb{E}[y]\mathbb{E}[x] + \mathbb{E}[x]\mathbb{E}[y]\\
= \sum_{x,y}p(x,y)xy - \mathbb{E}[x]\mathbb{E}[y]\\
= \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}[y]
$$


もし、

$$
\sum_{x,y}p(x,y)xy = \sum_{x,y}(x)p(y)xy = \left(\sum_xp(x)x\right)\left(\sum_yp(y)y\right)
$$

と書けるなら、$\rm{cov}[x,y] = 0$.\
$p(x,y) = p(x)p(y)$と書けることは$x,y$が独立であることに他ならない(演習1.6)。

### 1.2.3 ベイズ確率

- パラメータベクトル: $\bm{w}$
- データ: $\mathcal{D}$
  - 本文中の書体は筆記体: `$\mathcal{D}$`
- パラメータベクトルの事前確率: $p(\bm{w})$
- データが観測されたデータになる確率: $p\mathcal{D})$
- パラメータ$\bm{w}$のもとでのデータの実現確率 = 尤度関数: $p(\mathcal{D} | \bm{w})$
  - 確率変数は$\mathcal{D}$なので、$\bm{w}$による積分は意味を持たない。
  - **最尤推定**: 尤度関数$p(\mathcal{D} | \bm{w})$を最大にするパラメータベクトル$\bm{w}$を結果とする。
  - (機械学習業界における)誤差関数: $-\log p(\mathcal{D} | \bm{w})$
    - 尤度関数の最大化 <=> 誤差関数の最小化
- データ$\mathcal{D}$のもとでのパラメータベクトルの実現確率 = 事後確率: $p(\bm{w} | \mathcal{D}) = p(\mathcal{D} | \bm{w}) p(\bm{w}) / p(\mathcal{D})$

事後確率$p(\bm{w} | \mathcal{D})$の確率変数は$\bm{w}$なので、$\bm{w}$による積分値(連続分布の場合)は1になるはず。よって、

$$
\int \frac{p(\mathcal{D} | \bm{w}) p(\bm{w})} {p(\mathcal{D})}d\bm{w}\\
= \frac{\int p(\mathcal{D} | \bm{w}) p(\bm{w}) d\bm{w} } {p(\mathcal{D})} = 1.\\
\rightarrow \int p(\mathcal{D} | \bm{w}) p(\bm{w}) d\bm{w} = p(\mathcal{D}).
$$

となるはず。

重要そうな引用:

> ベイズ法を完全に実行するには特に全パラメータ空間での周辺化（和または積分）を必要としたからである。\
> これらの操作は後から見るように、予測を行ったり、異なるモデルを比較したりするために必要なものである。

### 1.2.4 ガウス分布

$(\infty,+\infty)$上の連続な分布関数。

$$
\mathcal{N}(x | \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right).
$$

#### 規格化

$$
\int^{+\infty}_{-\infty}\mathcal{N}(x | \mu,\sigma^2)dx
= \int^{+\infty}_{-\infty}\mathcal{N}(x | 0 ,\sigma^2)dx
= \int^{+\infty}_{-\infty}\mathcal{N}(x | 0 ,1)dx \\
= \int^{+\infty}_{-\infty} \frac{1}{\sqrt{2\pi}}\exp\left(-x^2/2\right)dx
$$

$x$と独立な変数$y$に対する正規分布を考え、2つの正規分布の積を求めることにすると、

$$
\int^{+\infty}_{-\infty}\mathcal{N}(x | \mu,\sigma^2)dx \int^{+\infty}_{-\infty}\mathcal{N}(y | \mu,\sigma^2)dy\\
= \int^{+\infty}_{-\infty} \frac{1}{\sqrt{2\pi}}\exp\left(-x^2/2\right)dx
\int^{+\infty}_{-\infty} \frac{1}{\sqrt{2\pi}}\exp\left(-y^2/2\right)dy \\
= \int^{+\infty}_{-\infty}\int^{+\infty}_{-\infty}dxdy \frac{1}{2\pi}\exp\left(-(x^2+y^2)/2\right)
$$

この積分を極座標に変換すると、

$$
\int^{+\infty}_{-\infty}\int^{+\infty}_{-\infty}dxdy \frac{1}{2\pi}\exp\left(-(x^2+y^2)/2\right)\\
= \int^{+\infty}_{0}dr\int^{+\pi}_{-\pi}\phi \frac{r}{2\pi}\exp\left(-r^2/2\right)\\
= \int^{+\infty}_{0}dr r\exp\left(-r^2/2\right)
$$

ここで、

$$
\frac{d}{dr}\exp\left(-r^2/2\right) = -r\exp\left(-r^2/2\right)
$$

であることを考慮すると、

$$
\int^{+\infty}_{0}dr r\exp\left(-r^2/2\right) = \left[-\exp\left(-r^2/2\right)\right]^{+\infty}_{0} = \exp(0) - 0 = 1.
$$

(演習1.7)

#### 期待値

$$
-\sigma^2\frac{d}{dx}\exp\left(-\frac{1}{2\sigma^2}x^2\right) = x\exp\left(-\frac{1}{2\sigma^2}x^2\right)
$$

であることを考えると、

$$
\int^{+\infty}_{-\infty}x \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)\\
= \int^{+\infty}_{-\infty}(x+\mu) \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}x^2) \\
= \int^{+\infty}_{-\infty}x\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}x^2)
+ \int^{+\infty}_{-\infty}\mu\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}x^2)\\
= \left[-\sigma^2\exp(-\frac{1}{2\sigma^2}x^2)\right]^{+\infty}_{-\infty} + \mu = (0 - 0) + \mu = \mu.
$$

#### 分散

$$
\int^{+\infty}_{-\infty}(x-\mu)^2 \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)dx\\
=  \int^{+\infty}_{-\infty}x^2 \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}x^2\right)dx\\.
$$

ここで、

$$
-\sigma^2\frac{d^2}{dx^2}\exp\left(-\frac{1}{2\sigma^2}x^2\right) = \frac{d}{dx} x\exp\left(-\frac{1}{2\sigma^2}x^2\right)\\
= \exp\left(-\frac{1}{2\sigma^2}x^2\right) - \frac{x^2}{\sigma^2}\exp\left(-\frac{1}{2\sigma^2}x^2\right)\\
\rightarrow 
-\sigma^2\frac{d}{dx}x\exp\left(-\frac{1}{2\sigma^2}x^2\right) + \sigma^2\exp\left(-\frac{1}{2\sigma^2}x^2\right) = x^2\exp\left(-\frac{1}{2\sigma^2}x^2\right)\\
$$

から、

$$
\int^{+\infty}_{-\infty}x^2 \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}x^2\right)\\
= \int^{+\infty}_{-\infty}\sigma^2\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}x^2\right) - \sigma^2 \left[x\exp\left(-\frac{1}{2\sigma^2}x^2\right)\right]^{+\infty}_{-\infty}\\
= \sigma^2.
$$


#### モード

分布関数$f(x)$の最大値を与える$x$の値のこと(argmax)。

明らかに $\argmax \mathcal{N}(x | \mu, \sigma^2) = \mu.$

#### 多変量ガウス分布

$$
\mathcal{N}(\mathrm{x} | \mathrm{\mu}, \mathrm{\Sigma}) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\mathrm{\Sigma}|^{1/2}}\exp\left( -\frac{1}{2}(\mathrm{x} - \mathrm{\mu})^\mathrm{T} \mathrm{\Sigma}^{-1} (\mathrm{x} - \mathrm{\mu}) \right)
$$

- $D$: 次元
- $\mathrm{x}$: $D$次元の連続変数ベクトル
- $\mathrm{\mu}$: $D$次元の平均ベクトル
- $\mathrm{\Sigma}$: $D \times D$次元の共分散行列
- $|\mathrm{\Sigma}|$: 共分散行列の行列式


(疑問) $\exp$内の$\mathrm{\Sigma}$はなぜ逆行列？

#### 問題設定

ある平均$\mu$と分散$\sigma^2$を持つガウス分布から生成されるデータ列$\rm{x}$を考える。

2つの別個のデータが"同じ分布"(例えばモデルもパラメータも等しいなら同じ分布)から生成される時、それらを**独立同分布**(i.i.d.)であるという。

データ列$\rm{x} = \{ x_1, x_2, ..., x_N \}$, つまり事象$x_1 \cap x_2 \cap ... \cap x_N$が実現される確率は、
各$x_i$が独立であることから

$$
p(x_1 \cap x_2 \cap ... \cap x_N) = \prod_{i=1..N} p(x_i) = \prod_{i=1..N}\mathcal{N}(x_i | \mu, \sigma^2)
$$

と書ける。\
これを最大化するようなパラメータを求めてみる。

#### 最尤推定

$$
\prod_{i=1..N}\mathcal{N}(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}^N}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1..N}(x_i - \mu)^2 \right)
$$

自然対数を取ると、

$$
\ln\prod_{i=1..N}\mathcal{N}(x_i | \mu, \sigma^2) = -\frac{N}{2}\left(\ln 2\pi + \ln\sigma^2\right) - \frac{1}{2\sigma^2}\sum_{i=1..N}(x_i - \mu)^2 = f(\mu, \sigma^2)
$$

微分を取ると、

$$
\frac{\partial f}{\partial  \mu} = \frac{1}{\sigma^2}\sum_{i=1..N}(x_i - \mu) = 0\\
\frac{\partial f}{\partial  \sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1..N}(x_i - \mu)^2 = 0\\
$$

両者を0にするような$(\mu, \sigma^2)$を求めると。

$$
\mu = \frac{1}{N}\sum_{i=1..N}x_i = \mu_{\rm{ML}}\\
\sigma^2 = \frac{1}{N}\sum_{i=1..N}(x_i - \mu_{\rm{ML}})^2 = \sigma^2_{\rm{ML}}
$$

$\rm{ML}$はそれぞれ(分布ではなく)**サンプルに対する**平均・分散であることを示している。\
(本当はこれが極小値を与えることを確かめないとダメ)

#### 不偏分散

サンプル平均$\mu_{\rm{ML}}$は分布$\mathcal{N}( \rm{x} | \mu, \sigma^2)$に従う確率変数なので、期待値が計算できる:

$$
\mathbb{E}[\mu_{\rm{ML}}] = \int \frac{1}{N}\sum_{i=1..N}x_i \mathcal{N}( \rm{x} | \mu, \sigma^2)dx = \frac{1}{N}\sum_{i=1..N}\mu = \mu.
$$

サンプル分散$\sigma^2_{\rm{ML}}$についても同様に期待値を計算してみる。\
まず

$$
\frac{1}{N}\sum_{i=1..N}(x_i - \mu_{\rm{ML}})^2
= \frac{1}{N}\sum_{i=1..N} \left( x_i - \frac{1}{N}\sum_{j=1..N}x_j \right)^2
= \frac{1}{N}\sum_{i=1..N} \left( x_i^2 - \frac{2x_i}{N}\sum_{j=1..N}x_j + \frac{1}{N^2}\left(\sum_{j=1..N}x_j \right)^2\right)
$$


第1項の期待値は$\sigma^2$.\
第2項は、各$i$について、

- $j = i$の成分(1個分): $-\frac{2}{N}\sigma^2$
- $j \ne i$の成分($N-1$個分): $-\frac{2}{N}\mu\sum_{j \ne i}x_j = -\frac{2}{N}\mu(N\mu - x_i)$
- 両者を足すと、$-\frac{2}{N}\left( \sigma^2 + N\mu^2 - \mu x_i \right)$

$i$について和を取ると、 $-2\left( \sigma^2 + (N-1)\mu^2\right)$.

第3項は、各$i$について、

- $i$を2つ含む成分(1個): $\frac{1}{N^2}\sigma^2$
- $i$を1つ含む成分($2N-2$個): $\frac{1}{N^2}\mu\sum_{j \ne i}x_j = \frac{1}{N^2}\mu(N\mu - x_i)$.
- $i$を含まない成分($N^2 - 2N + 1$個): $\frac{1}{N^2}\sum_{j \ne i, k \ne i}x_jx_k = \frac{1}{N^2}(N\mu - x_i)^2$.

(かきかけ)